import pandas as pd

# 예시 데이터
data = {'Name': ['A', 'B', 'C', 'D', 'E'],
        'Age': [25, 30, 22, 28, 24],
        'City': ['Busan', 'Seoul', 'Daegu', 'Jeju', 'Suwon']}

# 데이터프레임 생성
df = pd.DataFrame(data)

# 처음 3개의 행 출력
print(df.head(3))

df.info()

print(df.describe())

import pandas as pd
import numpy as np

# 예시 데이터
data = {'A': [1, 2, np.nan, 4, 5],
        'B': [np.nan, 10, 11, 12, np.nan],
        'C': [20, 21, 22, np.nan, 24]}

# 데이터프레임
df = pd.DataFrame(data)

# 결측치를 평균값으로 대체
df_filled = df.fillna(df.mean())

print("원본 데이터")
print(df)
print("결측치 처리 후 데이터")
print(df_filled)

import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler

# 붓꽃 데이터셋 로드
iris = load_iris()
iris_data = pd.DataFrame(data=iris.data, columns=iris.feature_names)
iris_target = pd.DataFrame(data=iris.target, columns=['species'])

# 데이터와 레이블 분리
X = iris_data  # 특성 데이터
y = iris_target['species']  # 레이블

iris_data

iris_target

# iris data
X = iris_data  # 특성 데이터
y = iris_target['species']  # 레이블

# 훈련 데이터와 테스트 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)


from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# 객체 생성
knn_classifier = KNeighborsClassifier(n_neighbors=3)

# 모델 학습
knn_classifier.fit(X_train, y_train)

# 예측
y_pred = knn_classifier.predict(X_test)

from sklearn.metrics import accuracy_score, recall_score, f1_score, roc_curve, auc

# 예시 데이터
y_true = [1, 0, 1, 1, 0, 1, 0, 1, 1, 0] # 실제값
y_pred = [1, 0, 0, 1, 0, 1, 0, 1, 0, 1] # 예측값
# Accuracy 계산
accuracy = accuracy_score(y_true, y_pred)
# Recall 계산
recall = recall_score(y_true, y_pred)
# F1 스코어 계산
f1 = f1_score(y_true, y_pred)
# ROC 곡선 및 AUC 계산
fpr, tpr, thresholds = roc_curve(y_true, y_pred)
roc_auc = auc(fpr, tpr)


# 결과 출력
print("Accuracy:", accuracy)
print("Recall:", recall)
print("F1 Score:", f1)
print("ROC AUC:", roc_auc)

import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# 예시 데이터
y_true = [3, 5, 7, 9, 11]
y_pred = [2.8, 5.2, 6.8, 9.3, 11.5]

# MAE 계산
mae = mean_absolute_error(y_true, y_pred)
# MSE 계산
mse = mean_squared_error(y_true, y_pred)
# RMSE 계산
rmse = np.sqrt(mse)
# R2 계산
r2 = r2_score(y_true, y_pred)

print("MAE:", mae)
print("MSE:", mse)
print("RMSE:", rmse)
print("R2:", r2)


import numpy as np
import pandas as pd
import seaborn as sns

bike = pd.read_csv('hour.csv')

bike.head(5)

bike.columns

bike.info()

bike.describe()

# 기록정보(instant) 인덱스 제거
bike = bike.drop(['instant'], axis = 1)
bike.head(5)

#날짜 인덱스 제거
bike = bike.drop(['dteday'], axis = 1)
bike.head(5)

#대여횟수 인덱스 제거
bike = bike.drop(['casual','registered'], axis = 1)
bike.head(5)

# 범주형 데이터 목록
category_list = ['season', 'hr', 'weekday','weathersit','workingday']

#백업 데이터 생성
bike_dummy = bike.copy()

from sklearn.model_selection import train_test_split

y_target_d = bike_dummy['cnt']
X_features_d = bike_dummy.drop(['cnt'], axis = 1, inplace = False)

X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X_features_d, y_target_d, test_size=0.3, random_state=1)

from sklearn.linear_model import LinearRegression

lr = LinearRegression()

lr.fit(X_train_d, y_train_d)

pred = lr.predict(X_test_d)

from sklearn.metrics import mean_squared_error , r2_score
mse = mean_squared_error(y_test_d, pred)

# RSME 방식
rmse = np.sqrt(mse)
print('MSE : {0:.3f} , RMSE : {1:.3F}'.format(mse, rmse))

#
print('r2_score : {0:.3f}'.format(r2_score(y_test_d, pred)))

import pandas as pd

iris = pd.read_csv('iris.csv')
iris

iris.columns

iris.info()

iris.describe()

# X와 Y분할
X = iris.iloc[:, :-1]
Y = iris.iloc[:, -1]

class_name = ['virginica', 'setosa', 'versicolor']

from sklearn.model_selection import train_test_split
# 데이터의 분할
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 0, stratify = Y)

# Class 별 분리 확인
print(y_train.value_counts()/y_train.shape[0] * 100)

# 의사 결정 나무 불러오기
from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier(criterion='entropy', random_state=1)

#트리에 넣어 학습
model.fit(X_train , y_train)

# 테스트 데이터에 대한 예측
y_pred = model.predict(X_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# 평가지표 계산
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='macro')
recall = recall_score(y_test, y_pred, average='macro')
f1 = f1_score(y_test, y_pred, average='macro')
roc_auc = roc_auc_score(y_test, model.predict_proba(X_test), multi_class='ovr')

# 평가지표 출력
print(f"Accuracy: {accuracy:.3f}")
print(f"Precision: {precision:.3f}")
print(f"Recall: {recall:.3f}")
print(f"F1 Score: {f1:.3f}")
print(f"AUC-ROC Score: {roc_auc:.3f}")

import pandas as pd
iris = pd.read_csv('iris.csv')

iris

# X와 Y분할
X = iris.iloc[:, :-1]
Y = iris.iloc[:, -1]

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 0, stratify = Y)

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(random_state = 0)

# 랜덤 포레스트 학습
rf.fit(X_train, y_train)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# 모델 예측
y_pred = rf.predict(X_test)

# 평가지표 계산
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='macro')
recall = recall_score(y_test, y_pred, average='macro')
f1 = f1_score(y_test, y_pred, average='macro')
roc_auc = roc_auc_score(y_test, rf.predict_proba(X_test), multi_class='ovr')

print(f"Accuracy: {accuracy:.3f}")
print(f"Precision: {precision:.3f}")
print(f"Recall: {recall:.3f}")
print(f"F1 Score: {f1:.3f}")


rf_1 = RandomForestClassifier(n_estimators = 10,   # 10번 추정
                              max_depth = 2,       # 트리 최대 깊이 2
                              random_state = 1)

rf_1.fit(X_train, y_train)

# 예측 결과를 pred_1 변수에 담기
pred_1 = rf_1.predict(X_test)
accuracy = accuracy_score(y_test, pred_1)

accuracy

rf_2 = RandomForestClassifier(n_estimators = 50,   # 500번 추정
                              max_depth = 3,        # 트리 최대 깊이 3
                              random_state = 1)

rf_2.fit(X_train, y_train)

pred_2 = rf_2.predict(X_test)
accuracy = accuracy_score(y_test, pred_2)

accuracy

import pandas as pd
from sklearn.linear_model import LogisticRegression

x_train = pd.read_csv('X_train_Boston.csv')
y_train = pd.read_csv('y_train_Boston.csv')

# 평가용 데이터 로드
x_test = pd.read_csv('X_test_Boston.csv')

X_train.info()

from sklearn.model_selection import train_test_split
# 훈련 데이터와 테스트 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=42)

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)


from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)

# 학습용 데이터에 대한 예측
y_train_pred = model.predict(X_train)
# 평가용 데이터에 대한 예측
y_test_pred = model.predict(x_test)[:81]

import numpy as np
from sklearn.metrics import mean_squared_error , r2_score

#MSE, RSME
mse = mean_squared_error(y_test, y_test_pred)
rmse = np.sqrt(mse)
print('MSE : {0:.3f} , RMSE : {1:.3F}'.format(mse, rmse))


import pandas as pd
import matplotlib.pyplot as plt

# 예측 결과 시각화
plt.figure(figsize=(10, 6))

plt.scatter(y_train, y_train_pred, color='blue', alpha=0.5, label='Training Data')
plt.scatter(y_test, y_test_pred, color='red', alpha=0.5, label='Test Data')

plt.plot([0, 50], [0, 50], 'k--')

plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs. Predicted Values')
plt.legend()
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 타이타닉 데이터셋 로드
x_train = pd.read_csv('X_train.csv')
y_train = pd.read_csv('y_train.csv')
x_test = pd.read_csv('X_test.csv')

x_train.info()

from sklearn.model_selection import train_test_split

# 훈련 데이터와 테스트 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=42)

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)


from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier()
model.fit(X_train, y_train)

# 평가용 데이터에 대한 예측
y_pred = model.predict(x_test)[:143]

accuracy = accuracy_score(y_test, y_pred)
print(f"정확도: {accuracy:.2f}")

import xgboost as xgb

# 학습용 데이터 로드
x_train = pd.read_csv('X_train_winequality.csv')
y_train = pd.read_csv('y_train_winequality.csv')

# 평가용 데이터 로드
x_test = pd.read_csv('X_test_winequality.csv')

x_train.info()

x_train.describe()

import pandas as pd
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

# 데이터 정규화
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.fit_transform(x_test)
y_train_scaled = scaler.fit_transform(y_train)

# 정규화된 데이터를 DataFrame으로 변환
x_train_scaled = pd.DataFrame(x_train_scaled, columns=x_train.columns)
x_test_scaled = pd.DataFrame(x_test_scaled, columns=x_test.columns)
y_train_scaled = pd.DataFrame(y_train_scaled, columns=y_train.columns)
x_train_scaled

from sklearn.model_selection import train_test_split
# 훈련 데이터와 테스트 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(x_train_scaled, y_train, test_size=0.2, random_state=42)

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)


import xgboost as xgb

model = xgb.XGBClassifier(objective='multi:softmax', num_class=6, random_state=42)

model.fit(X_train, y_train)

from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, model.predict(x_test_scaled)[:256])

print(accuracy)

import numpy as np
import pandas as pd

x_train = pd.read_csv('X_train_mushroom.csv')
y_train = pd.read_csv('y_train_mushroom.csv')
x_test = pd.read_csv('X_test_mushroom.csv')

x_train.info()


from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()

for col in x_train.columns:
    x_train[col] = label_encoder.fit_transform(x_train[col])
    x_test[col] = label_encoder.transform(x_test[col])

x_train.info()

y_train = pd.get_dummies(y_train['class'])

y_train

from sklearn.model_selection import train_test_split
# 훈련 데이터와 테스트 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=42)

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)


from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

from sklearn.metrics import accuracy_score, f1_score

# F1 스코어 계산
f1 = f1_score(y_test.values.argmax(axis=1), y_pred.argmax(axis=1), average='weighted')
print(f1)

import pandas as pd
# CSV 파일 로드
cancer = pd.read_csv('cancer.csv')

cancer.columns

cancer.info()

cancer.drop('id', axis=1, inplace=True)

summary_stats = cancer.describe()

# 'std' 행을 기준으로 내림차순으로 정렬
sorted_summary_stats = summary_stats.sort_values(by='std', axis=1, ascending=False)

print(sorted_summary_stats)


from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

cancer[['area_worst', 'area_mean']] = scaler.fit_transform(cancer[['area_worst', 'area_mean']])

cancer.diagnosis

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
cancer['diagnosis'] = label_encoder.fit_transform(cancer['diagnosis'])

cancer.diagnosis

from sklearn.model_selection import train_test_split

X = cancer.drop('diagnosis', axis=1)
y = cancer['diagnosis']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.neighbors import KNeighborsClassifier

knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, y_train)

y_pred = knn_model.predict(X_test)

from sklearn.metrics import accuracy_score

# 평가 결과 출력
print(accuracy_score(y_test, y_pred))
